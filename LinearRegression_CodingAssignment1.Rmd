---
title: "Linear Regression Coding Assignment-1 - 231058001 - Nidhi S"
editor_options:
  chunk_output_type: inline
output:
  pdf_document: default
  html_notebook: default
---

```{r}
# Load essential libraries
library(ggplot2)
library(dplyr)
```


```{r}
# Load the house price dataset
hData = read.csv('Data/houseprices_cleaned.csv', header =TRUE, na.strings=c('NA',"",'Not Available'))
str(hData)
```

```{r}
# Convert 'locality', 'facing' and 'parking' columns to factors
categorical_cols = c('locality', 'facing', 'parking')
hData[categorical_cols] = lapply(hData[categorical_cols], as.factor)
str(hData)
```

```{r}
# Continuous columns
continuous_cols = setdiff(colnames(hData),categorical_cols)
continuous_cols
```

```{r}
# Plot percentage of NAs in each column of the data frame
hData_NA = setNames(stack(sapply(hData, function(x){(sum(is.na(x))/length(x))*100}))[2:1], c('Feature','Value'))
p = ggplot(data = hData_NA, aes(x = Feature, y = Value)) +
  geom_bar(stat = 'identity', fill = 'steelblue', width = 0.3) +
  theme(text = element_text(size = 14, face = 'bold'),
  axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  xlab('') + ylab('Percentage') +
  ggtitle('Percentage of NAs across all features')
p
```

```{r}
# Add NA as a factor level for categorical columns 
hData[categorical_cols]=lapply(hData[categorical_cols],addNA)
str(hData)
```

```{r}
# Make a histogram of rent values
p = ggplot(data = hData) +
  geom_histogram(aes(x = rent, y = after_stat(count)), breaks = seq(mean(hData$rent)-4*sd(hData$rent), mean(hData$rent)+4*sd(hData$rent), by = 25000), color = 'black', fill = 'blue') +
  labs(x = 'Rent', y = 'Frequency')  +
   theme(axis.text = element_text(size = 8),
   axis.text.x = element_text(size = 10),
   axis.text.y = element_text(size = 10),
   axis.title = element_text(size = 10, face = "bold"))  +
  ggtitle('Histogram of house rents')
p
```

```{r}
# Build a linear model to predict price per square feet as a function of rent. How accurate is the model?
model = lm(data = hData,price_per_sqft~rent)
summary(model)
```

```{r}
# Make a histogram of log-transformed rent values
hData['logrent'] = log(hData$rent)
p = ggplot(data = hData) +
geom_histogram(aes(x = logrent, y = after_stat(count)), breaks = seq(mean(hData$logrent)-4*sd(hData$logrent), mean(hData$logrent)+4*sd(hData$logrent), by = 0.5), color = 'black', fill = 'blue') +
  labs(x = 'Rent', y = 'Frequency')  +
   theme(axis.text = element_text(size = 8),
   axis.text.x = element_text(size = 10),
   axis.text.y = element_text(size = 10),
   axis.title = element_text(size = 10, face = "bold"))  +
  ggtitle('Histogram of house logrents')
p

```

```{r}
# Build a linear model to predict price per square feet as a function of logrent. Did log-transforming rent help improve the model accuracy?
model = lm(data=hData,price_per_sqft~logrent)
summary(model)

```
## Yes log transforming has imporved the model accuracy. It was previously 55%, now its increased to almost 68%

```{r}
# Build a linear model to predict log of price per square feet as a function of logrent. Did log-transforming the response variable price per square feet improve the model accuracy?
hData['logprice_per_sqft'] = log(hData$price_per_sqft)
model = lm(data = hData,logprice_per_sqft~logrent)
summary(model)
```

## NO log-transforming the response variable price per square feet has not improved the accuracy of the model. The R-squared value is still the same as previous model.
```{r}
# Build a linear model to predict sqrt of price per square feet as a function of logrent. Did sqrt-transforming the response variable price per square feet improve the model accuracy?
hData['sqrtprice_per_sqft'] = sqrt(hData$price_per_sqft)
model = lm(data=hData, sqrtprice_per_sqft~logrent)
summary(model)

```
##### Yes , sqrt-transforming the response variable price per square feethas improved the accuracy by almost 1%.

```{r}
# Build a linear model to predict price per sqft as a function of area and rent. Did adding area as an additional predictor improve model accuracy (compared to only rent as the predictor)? Also, interpret the coefficient estimates for area and rent practically.
model = lm(data=hData, price_per_sqft~area+rent)
summary(model)

```
##### Yes adding area as an additional predictor improves the model accuracy. We can see both the R-squared value as well as the Adjusted R-Squared value have been increase to 73%.
#### price(predicted)=beta0(predicted)+beta1(predicted)*area+beta2(predicted)*rent
#### price(predicted)=6455-2.521*area+0.06653*rent
#### it tells for one unit increase in the rent, the price will get increased by 0.06653 units and for one unit increase in the area, the price may get decreased by 2.521 units.
```{r}
# Build a linear model to predict sqrt of price per sqft as a function of area and logrent. Did adding area as an additional predictor improve model accuracy (compared to only logrent as the predictor)? Also, interpret the coefficient estimates for area and logrent practically.
model = lm(data=hData, sqrtprice_per_sqft~area+logrent)
summary(model)

```
##### Yes adding area as an additional predictor improves the model accuracy. We can see both the R-squared value as well as the Adjusted R-Squared value have been increase to 87%.
#### price(predicted)=beta0(predicted)+beta1(predicted)*area+beta2(predicted)*logrent
#### price(predicted)=-2832-0.01307*area+31.47*logrent

```{r}
# Build a linear model to predict sqrt of price per sqft as a function of logarea and logrent. Did log-transforming area improve model accuracy?
hData['logarea'] = log(hData$area)
model = lm(data=hData, sqrtprice_per_sqft~logarea+logrent)
summary(model)

```
#### log-transforming area improve model accuracy improved the accuracy to almost 98%

```{r}
# Build a linear model to predict price per sqft as a function of area, rent, and parking (compared to just using area and rent as predictors). Did adding parking as an additional predictor improve model accuracy?
model = lm(data=hData, price_per_sqft~area+rent+parking)
summary(model)

```
#### No adding this additional predictor Parking has actually reduced the accuracy of the model

```{r}
# Build a linear model to predict sqrt of price per sqft as a function of logarea, logrent, and locality. Did adding locality as an additional predictor improve model accuracy (compared to just using logarea and logrent as predictors)?
model = lm(data=hData, sqrtprice_per_sqft~logarea+logrent+locality)
summary(model)

```
#### Yes it improved the accuracy of the model
```{r}
# Build a linear model to predict price per sqft as a function of area, rent, and parking. How many levels does the categorical feature parking have? How many new variables are introduced for the categorical variable parking? Interpret all regression coefficient estimates except the intercept coefficient estimate beta0 practically. Do the p-values suggest any insignificant features (that is, features which probably don't have a linear relationship with the response variable?
model = lm(data=hData, price_per_sqft~area+rent+parking)
summary(model)
```
#### categorical feature parking has 4 levels(Bike,Bike and Car, Car,NA). New 3 variables(ParkingBike and Car, ParkingCar,ParkingNA) are introduced. 
#### price(predicted)=5860-2.453*area+0.006578*rent+53*(ParkingBike and Car)+88.63*(ParkingCar)+27.24*ParkingNA
#### This interprets that if the area increases by one unit then the rent decreases by 2.453 units.
#### If the rent increases by 1 unit then the price also increases by 0.006578 units.
#### If the house has Parking for Bike and Car then the price increases by 53 units
#### IF the house has parking for only car then the price increases by 88.6 units

#### By looking at the p-values we can say that the p-values for the parking reference variables(excluding last reference as the last reference level will always have the higher p-values) other p-values are also not very small if we compare with other predictors. So we failed to reject the null hypotheses i.e, price has no linear relationship with the parking.








```{r}
# Create new columns corresponding to scaled versions of the continuous columns
hData[paste0('scaled_', continuous_cols)] = lapply(hData[continuous_cols], scale)
str(hData)
```

```{r}
# Build a linear model to predict scaled price per sqft as a function of scaled area and scaled rent. Compare this with the model built using unscaled data: that is, predict price per sqft as a function of area and rent. Does scaling help?
model = lm(data=hData, scaled_price_per_sqft~scaled_area+scaled_rent)
summary(model)

```
#### No, it doesn't improve the accuracy of the model. The R-squared value of the model with both scaled and unscaled data are same.
```{r}
# Rebuild a linear model to predict sqrt of price per sqft as a function of logarea, logrent, and locality which we will evaluate using a train-test split of the dataset
model = lm(data = hData, sqrtprice_per_sqft ~ logarea + logrent + locality)
summary(model)

```

```{r}
# Split data into train (80%) and test (20%) sets and evaluate model performance on train and test sets. Run this cell multiple times for a random splitting of the data into train and test sets and report the model performance on the resulting train and test sets. Is there much variability in the model performance across different test sets? If that is the case, then the model is not generalizing well and is overfitting the train set. Is it the case here?
ind = sample(nrow(hData), size = floor(0.8*nrow(hData)), replace = FALSE)
hData_train = hData[ind, ]
hData_test = hData[-ind, ]

model = lm(data=hData_train,scaled_price_per_sqft~scaled_area+scaled_rent+locality)

# Root Mean Squared Error (RMSE) on train data
train_error <- sqrt(mean((hData_train$scaled_price_per_sqft  - predict(model, newdata = hData_train))^2))

# Calculate RSE (root-mean-squared-error) on test data
test_error = sqrt(mean((hData_test$scaled_price_per_sqft  - predict(model, newdata = hData_test))^2))

print(train_error)
print(test_error)
```




```{r}
ind = sample(nrow(hData), size = floor(0.8*nrow(hData)), replace = FALSE)
hData_train = hData[ind, ]
hData_test = hData[-ind, ]

model = lm(data=hData_train,scaled_price_per_sqft~scaled_area+scaled_rent+locality)

# Root Mean Squared Error (RMSE) on train data
train_error <- sqrt(mean((hData_train$scaled_price_per_sqft  - predict(model, newdata = hData_train))^2))

# Calculate RSE (root-mean-squared-error) on test data
test_error = sqrt(mean((hData_test$scaled_price_per_sqft  - predict(model, newdata = hData_test))^2))

print(train_error)
print(test_error)
```
```{r}
ind = sample(nrow(hData), size = floor(0.8*nrow(hData)), replace = FALSE)
hData_train = hData[ind, ]
hData_test = hData[-ind, ]

model = lm(data=hData_train,scaled_price_per_sqft~scaled_area+scaled_rent+locality)

# Root Mean Squared Error (RMSE) on train data
train_error <- sqrt(mean((hData_train$scaled_price_per_sqft  - predict(model, newdata = hData_train))^2))

# Calculate RSE (root-mean-squared-error) on test data
test_error = sqrt(mean((hData_test$scaled_price_per_sqft  - predict(model, newdata = hData_test))^2))

print(train_error)
print(test_error)
```


```{r}
ind = sample(nrow(hData), size = floor(0.8*nrow(hData)), replace = FALSE)
hData_train = hData[ind, ]
hData_test = hData[-ind, ]

model = lm(data=hData_train,scaled_price_per_sqft~scaled_area+scaled_rent+locality)

# Root Mean Squared Error (RMSE) on train data
train_error <- sqrt(mean((hData_train$scaled_price_per_sqft  - predict(model, newdata = hData_train))^2))

# Calculate RSE (root-mean-squared-error) on test data
test_error = sqrt(mean((hData_test$scaled_price_per_sqft  - predict(model, newdata = hData_test))^2))

print(train_error)
print(test_error)
```
```{r}
ind = sample(nrow(hData), size = floor(0.8*nrow(hData)), replace = FALSE)
hData_train = hData[ind, ]
hData_test = hData[-ind, ]

model = lm(data=hData_train,scaled_price_per_sqft~scaled_area+scaled_rent+locality)

# Root Mean Squared Error (RMSE) on train data
train_error <- sqrt(mean((hData_train$scaled_price_per_sqft  - predict(model, newdata = hData_train))^2))

# Calculate RSE (root-mean-squared-error) on test data
test_error = sqrt(mean((hData_test$scaled_price_per_sqft  - predict(model, newdata = hData_test))^2))

print(train_error)
print(test_error)
```

```{r}
ind = sample(nrow(hData), size = floor(0.8*nrow(hData)), replace = FALSE)
hData_train = hData[ind, ]
hData_test = hData[-ind, ]

model =lm(data=hData_train,price_per_sqft~area+rent+locality)

# Root Mean Squared Error (RMSE) on train data
train_error = sqrt(mean((hData_train$price_per_sqft - predict(model, newdata = hData_train))^2))

# Calculate RSE (root-mean-squared-error) on test data
test_error = sqrt(mean((hData_test$price_per_sqft  - predict(model, newdata = hData_test))^2))

print(train_error)
print(test_error)
```

```{r}
ind = sample(nrow(hData), size = floor(0.8*nrow(hData)), replace = FALSE)
hData_train = hData[ind, ]
hData_test = hData[-ind, ]

model =lm(data=hData_train,price_per_sqft~area+rent+locality)

# Root Mean Squared Error (RMSE) on train data
train_error = sqrt(mean((hData_train$price_per_sqft - predict(model, newdata = hData_train))^2))

# Calculate RSE (root-mean-squared-error) on test data
test_error = sqrt(mean((hData_test$price_per_sqft  - predict(model, newdata = hData_test))^2))

print(train_error)
print(test_error)
```

```{r}
ind = sample(nrow(hData), size = floor(0.8*nrow(hData)), replace = FALSE)
hData_train = hData[ind, ]
hData_test = hData[-ind, ]

model =lm(data=hData_train,price_per_sqft~area+rent+locality)

# Root Mean Squared Error (RMSE) on train data
train_error = sqrt(mean((hData_train$price_per_sqft - predict(model, newdata = hData_train))^2))

# Calculate RSE (root-mean-squared-error) on test data
test_error = sqrt(mean((hData_test$price_per_sqft  - predict(model, newdata = hData_test))^2))

print(train_error)
print(test_error)
```

### The test error is getting varied for different test sets.